Score=[{
      "Model": "LLaVA",
      "COCO": 0.247,
      "C.C.": 0.227,
      "Diffusion": 0.06,
      "Graphics": 0.242,
      "Math": 0.093,
      "Text": 0.245,
      "WIT": 0.109,
      "Chart": 0.237,
      "VisIT": 0.177,
      "CC-3M": 0.071,
      "Average Score": 0.171
    },
    {
      "Model": "CogVLM",
      "COCO": 0.107,
      "C.C.": -0.048,
      "Diffusion": 0.049,
      "Graphics": -0.158,
      "Math": 0.065,
      "Text": 0.097,
      "WIT": -0.131,
      "Chart": -0.135,
      "VisIT": 0.278,
      "CC-3M": 0.157,
      "Average Score": 0.028
    },
    {
      "Model": "Gemini",
      "COCO": 0.262,
      "C.C.": 0.408,
      "Diffusion": null,
      "Graphics": 0.4,
      "Math": 0.228,
      "Text": 0.222,
      "WIT": 0.418,
      "Chart": 0.343,
      "VisIT": 0.336,
      "CC-3M": 0.374,
      "Average Score": 0.332
    },
    {
      "Model": "GPT-4V",
      "COCO": 0.454,
      "C.C.": 0.507,
      "Diffusion": 0.458,
      "Graphics": 0.645,
      "Math": 0.606,
      "Text": 0.624,
      "WIT": 0.579,
      "Chart": 0.645,
      "VisIT": 0.62,
      "CC-3M": 0.431,
      "Average Score": 0.557
    }
]
Pair_w.Tie=[
  {
    "Model": "LLaVA",
    "COCO": 0.273,
    "C.C.": 0.478,
    "Diffusion": 0.286,
    "Graphics": 0.273,
    "Math": 0.657,
    "Text": 0.51,
    "WIT": 0.369,
    "Chart": 0.383,
    "VisIT": 0.456,
    "CC-3M": 0.484,
    "Average Score": 0.417
  },
  {
    "Model": "CogVLM",
    "COCO": 0.548,
    "C.C.": 0.409,
    "Diffusion": 0.562,
    "Graphics": 0.613,
    "Math": 0.412,
    "Text": 0.25,
    "WIT": 0.273,
    "Chart": 0.262,
    "VisIT": 0.324,
    "CC-3M": 0.433,
    "Average Score": 0.409
  },
  {
    "Model": "Gemini",
    "COCO": 0.616,
    "C.C.": 0.787,
    "Diffusion": null,
    "Graphics": 0.65,
    "Math": 0.436,
    "Text": 0.664,
    "WIT": 0.605,
    "Chart": 0.5,
    "VisIT": 0.66,
    "CC-3M": 0.56,
    "Average Score": 0.609
  },
  {
    "Model": "GPT-4V",
    "COCO": 0.696,
    "C.C.": 0.824,
    "Diffusion": 0.847,
    "Graphics": 0.639,
    "Math": 0.564,
    "Text": 0.673,
    "WIT": 0.679,
    "Chart": 0.657,
    "VisIT": 0.64,
    "CC-3M": 0.612,
    "Average Score": 0.683
  }
]
Pair_w.o.Tie=[
    {
      "Model": "LLaVA",
      "COCO": 0.327,
      "C.C.": 0.537,
      "Diffusion": 0.302,
      "Graphics": 0.3,
      "Math": 0.726,
      "Text": 0.684,
      "WIT": 0.6,
      "Chart": 0.61,
      "VisIT": 0.648,
      "CC-3M": 0.583,
      "Average Score": 0.532
    },
    {
      "Model": "CogVLM",
      "COCO": 0.654,
      "C.C.": 0.45,
      "Diffusion": 0.643,
      "Graphics": 0.704,
      "Math": 0.481,
      "Text": 0.292,
      "WIT": 0.5,
      "Chart": 0.423,
      "VisIT": 0.5,
      "CC-3M": 0.591,
      "Average Score": 0.524
    },
    {
      "Model": "Gemini",
      "COCO": 0.717,
      "C.C.": 0.84,
      "Diffusion": null,
      "Graphics": 0.77,
      "Math": 0.678,
      "Text": 0.793,
      "WIT": 0.688,
      "Chart": 0.658,
      "VisIT": 0.711,
      "CC-3M": 0.652,
      "Average Score": 0.723
    },
    {
      "Model": "GPT-4V",
      "COCO": 0.804,
      "C.C.": 0.87,
      "Diffusion": 0.922,
      "Graphics": 0.807,
      "Math": 0.801,
      "Text": 0.805,
      "WIT": 0.734,
      "Chart": 0.849,
      "VisIT": 0.761,
      "CC-3M": 0.703,
      "Average Score": 0.806
    }
]

truthfulness2=[{"Model": "ChatGPT", "Internal (\u2191)": 0.288, "External  (\u2191) ": 0.726, "Hallucination  (\u2191)": 0.529, "Persona Sycophancy  (\u2193)": 0.039, "Preference Sycophancy  (\u2193)": 0.257, "Adv Factuality  (\u2191)": 0.708}, {"Model": "GPT-4", "Internal (\u2191) ": 0.417, "External  (\u2191)": 0.793, "Hallucination  (\u2191)": 0.516, "Persona Sycophancy  (\u2193)": 0.029, "Preference Sycophancy  (\u2193)": 0.296, "Adv Factuality  (\u2191)": 0.813}, {"Model": "ChatGLM2", "Internal (\u2191) ": 0.127, "External  (\u2191)": 0.542, "Hallucination  (\u2191)": 0.542, "Persona Sycophancy  (\u2193)": 0.036, "Preference Sycophancy  (\u2193)": 0.432, "Adv Factuality  (\u2191)": 0.349}, {"Model": "Vicuna-33b", "Internal (\u2191) ": 0.261, "External  (\u2191)": 0.726, "Hallucination  (\u2191)": 0.423, "Persona Sycophancy  (\u2193)": 0.038, "Preference Sycophancy  (\u2193)": 0.458, "Adv Factuality  (\u2191)": 0.699}, {"Model": "Vicuna-13b", "Internal (\u2191) ": 0.18, "External  (\u2191)": 0.623, "Hallucination  (\u2191)": 0.403, "Persona Sycophancy  (\u2193)": 0.036, "Preference Sycophancy  (\u2193)": 0.375, "Adv Factuality  (\u2191)": 0.665}, {"Model": "Vicuna-7b", "Internal (\u2191) ": 0.132, "External  (\u2191)": 0.581, "Hallucination  (\u2191)": 0.347, "Persona Sycophancy  (\u2193)": 0.03, "Preference Sycophancy  (\u2193)": 0.395, "Adv Factuality  (\u2191)": 0.469}, {"Model": "Llama2-70b", "Internal (\u2191) ": 0.313, "External  (\u2191)": 0.721, "Hallucination  (\u2191)": 0.402, "Persona Sycophancy  (\u2193)": 0.043, "Preference Sycophancy  (\u2193)": 0.468, "Adv Factuality  (\u2191)": 0.794}, {"Model": "Llama2-13b", "Internal (\u2191) ": 0.235, "External  (\u2191)": 0.722, "Hallucination  (\u2191)": 0.404, "Persona Sycophancy  (\u2193)": 0.032, "Preference Sycophancy  (\u2193)": 0.571, "Adv Factuality  (\u2191)": 0.78}, {"Model": "Llama2-7b", "Internal (\u2191) ": 0.203, "External  (\u2191)": 0.638, "Hallucination  (\u2191)": 0.396, "Persona Sycophancy  (\u2193)": 0.035, "Preference Sycophancy  (\u2193)": 0.587, "Adv Factuality  (\u2191)": 0.718}, {"Model": "Wizardlm-13b", "Internal (\u2191) ": 0.19, "External  (\u2191)": 0.574, "Hallucination  (\u2191)": 0.356, "Persona Sycophancy  (\u2193)": 0.025, "Preference Sycophancy  (\u2193)": 0.385, "Adv Factuality  (\u2191)": 0.794}, {"Model": "Koala-13b", "Internal (\u2191) ": 0.145, "External  (\u2191)": 0.553, "Hallucination  (\u2191)": 0.451, "Persona Sycophancy  (\u2193)": 0.04, "Preference Sycophancy  (\u2193)": 0.5, "Adv Factuality  (\u2191)": 0.435}, {"Model": "Baichuan-13b", "Internal (\u2191) ": 0.17, "External  (\u2191)": 0.622, "Hallucination  (\u2191)": 0.306, "Persona Sycophancy  (\u2193)": 0.032, "Preference Sycophancy  (\u2193)": 0.286, "Adv Factuality  (\u2191)": 0.44}, {"Model": "Oasst-12b", "Internal (\u2191) ": 0.101, "External  (\u2191)": 0.534, "Hallucination  (\u2191)": 0.418, "Persona Sycophancy  (\u2193)": 0.031, "Preference Sycophancy  (\u2193)": 0.436, "Adv Factuality  (\u2191)": 0.221}, {"Model": "ERNIE", "Internal (\u2191) ": 0.255, "External  (\u2191)": 0.689, "Hallucination  (\u2191)": 0.515, "Persona Sycophancy  (\u2193)": 0.019, "Preference Sycophancy  (\u2193)": 0.312, "Adv Factuality  (\u2191)": 0.407}, {"Model": "Mistral-7b", "Internal (\u2191) ": 0.341, "External  (\u2191)": 0.687, "Hallucination  (\u2191)": 0.458, "Persona Sycophancy  (\u2193)": 0.035, "Preference Sycophancy  (\u2193)": 0.293, "Adv Factuality  (\u2191)": 0.426}, {"Model": "PaLM2", "Internal (\u2191) ": 0.284, "External  (\u2191)": 0.532, "Hallucination  (\u2191)": 0.379, "Persona Sycophancy  (\u2193)": 0.028, "Preference Sycophancy  (\u2193)": 0.581, "Adv Factuality  (\u2191)": 0.273}]
truthfulness=[{"Model": "ChatGPT", "Internal (\u2191)": 0.288, "External  (\u2191)": 0.726, "Hallucination  (\u2191)": 0.529, "Persona Sycophancy  (\u2193)": 0.039, "Preference Sycophancy  (\u2193)": 0.257, "Adv Factuality  (\u2191)": 0.708}, {"Model": "GPT-4", "Internal (\u2191) ": 0.417, "External  (\u2191)": 0.793, "Hallucination  (\u2191)": 0.516, "Persona Sycophancy  (\u2193)": 0.029, "Preference Sycophancy  (\u2193)": 0.296, "Adv Factuality  (\u2191)": 0.813}, {"Model": "ChatGLM2", "Internal (\u2191) ": 0.127, "External  (\u2191)": 0.542, "Hallucination  (\u2191)": 0.542, "Persona Sycophancy  (\u2193)": 0.036, "Preference Sycophancy  (\u2193)": 0.432, "Adv Factuality  (\u2191)": 0.349}, {"Model": "Vicuna-33b", "Internal (\u2191) ": 0.261, "External  (\u2191)": 0.726, "Hallucination  (\u2191)": 0.423, "Persona Sycophancy  (\u2193)": 0.038, "Preference Sycophancy  (\u2193)": 0.458, "Adv Factuality  (\u2191)": 0.699}, {"Model": "Vicuna-13b", "Internal (\u2191) ": 0.18, "External  (\u2191)": 0.623, "Hallucination  (\u2191)": 0.403, "Persona Sycophancy  (\u2193)": 0.036, "Preference Sycophancy  (\u2193)": 0.375, "Adv Factuality  (\u2191)": 0.665}, {"Model": "Vicuna-7b", "Internal (\u2191) ": 0.132, "External  (\u2191)": 0.581, "Hallucination  (\u2191)": 0.347, "Persona Sycophancy  (\u2193)": 0.03, "Preference Sycophancy  (\u2193)": 0.395, "Adv Factuality  (\u2191)": 0.469}, {"Model": "Llama2-70b", "Internal (\u2191) ": 0.313, "External  (\u2191)": 0.721, "Hallucination  (\u2191)": 0.402, "Persona Sycophancy  (\u2193)": 0.043, "Preference Sycophancy  (\u2193)": 0.468, "Adv Factuality  (\u2191)": 0.794}, {"Model": "Llama2-13b", "Internal (\u2191) ": 0.235, "External  (\u2191)": 0.722, "Hallucination  (\u2191)": 0.404, "Persona Sycophancy  (\u2193)": 0.032, "Preference Sycophancy  (\u2193)": 0.571, "Adv Factuality  (\u2191)": 0.78}, {"Model": "Llama2-7b", "Internal (\u2191) ": 0.203, "External  (\u2191)": 0.638, "Hallucination  (\u2191)": 0.396, "Persona Sycophancy  (\u2193)": 0.035, "Preference Sycophancy  (\u2193)": 0.587, "Adv Factuality  (\u2191)": 0.718}, {"Model": "Wizardlm-13b", "Internal (\u2191) ": 0.19, "External  (\u2191)": 0.574, "Hallucination  (\u2191)": 0.356, "Persona Sycophancy  (\u2193)": 0.025, "Preference Sycophancy  (\u2193)": 0.385, "Adv Factuality  (\u2191)": 0.794}, {"Model": "Koala-13b", "Internal (\u2191) ": 0.145, "External  (\u2191)": 0.553, "Hallucination  (\u2191)": 0.451, "Persona Sycophancy  (\u2193)": 0.04, "Preference Sycophancy  (\u2193)": 0.5, "Adv Factuality  (\u2191)": 0.435}, {"Model": "Baichuan-13b", "Internal (\u2191) ": 0.17, "External  (\u2191)": 0.622, "Hallucination  (\u2191)": 0.306, "Persona Sycophancy  (\u2193)": 0.032, "Preference Sycophancy  (\u2193)": 0.286, "Adv Factuality  (\u2191)": 0.44}, {"Model": "Oasst-12b", "Internal (\u2191) ": 0.101, "External  (\u2191)": 0.534, "Hallucination  (\u2191)": 0.418, "Persona Sycophancy  (\u2193)": 0.031, "Preference Sycophancy  (\u2193)": 0.436, "Adv Factuality  (\u2191)": 0.221}, {"Model": "ERNIE", "Internal (\u2191) ": 0.255, "External  (\u2191)": 0.689, "Hallucination  (\u2191)": 0.515, "Persona Sycophancy  (\u2193)": 0.019, "Preference Sycophancy  (\u2193)": 0.312, "Adv Factuality  (\u2191)": 0.407}, {"Model": "Mistral-7b", "Internal (\u2191) ": 0.341, "External  (\u2191)": 0.687, "Hallucination  (\u2191)": 0.458, "Persona Sycophancy  (\u2193)": 0.035, "Preference Sycophancy  (\u2193)": 0.293, "Adv Factuality  (\u2191)": 0.426}, {"Model": "PaLM2", "Internal (\u2191) ": 0.284, "External  (\u2191)": 0.532, "Hallucination  (\u2191)": 0.379, "Persona Sycophancy  (\u2193)": 0.028, "Preference Sycophancy  (\u2193)": 0.581, "Adv Factuality  (\u2191)": 0.273}]
robustness=[{"Model": "Baichuan-13b", "AdvGlue RS (\u2191)": 0.363, "AdvInstruction (\u2191)": 93.99, "OOD detection (\u2191)": 0.004, "OOD generalization (\u2191)": 0.539}, {"Model": "ChatGLM2", "AdvGlue RS (\u2191)": 0.254, "AdvInstruction (\u2191)": 94.37, "OOD detection (\u2191)": 0.627, "OOD generalization (\u2191)": 0.778}, {"Model": "Vicuna-13b", "AdvGlue RS (\u2191)": 0.18, "AdvInstruction (\u2191)": 96.01, "OOD detection (\u2191)": 0.635, "OOD generalization (\u2191)": 0.839}, {"Model": "Vicuna-7b", "AdvGlue RS (\u2191)": 0.072, "AdvInstruction (\u2191)": 88.77, "OOD detection (\u2191)": 0.49, "OOD generalization (\u2191)": 0.753}, {"Model": "Vicuna-33b", "AdvGlue RS (\u2191)": 0.219, "AdvInstruction (\u2191)": 95.97, "OOD detection (\u2191)": 0.685, "OOD generalization (\u2191)": 0.785}, {"Model": "Llama2-7b", "AdvGlue RS (\u2191)": 0.374, "AdvInstruction (\u2191)": 96.68, "OOD detection (\u2191)": 0.465, "OOD generalization (\u2191)": 0.777}, {"Model": "Llama2-13b", "AdvGlue RS (\u2191)": 0.306, "AdvInstruction (\u2191)": 96.48, "OOD detection (\u2191)": 0.432, "OOD generalization (\u2191)": 0.884}, {"Model": "Koala-13b", "AdvGlue RS (\u2191)": 0.116, "AdvInstruction (\u2191)": 94.05, "OOD detection (\u2191)": 0.552, "OOD generalization (\u2191)": 0.584}, {"Model": "Oasst-12b", "AdvGlue RS (\u2191)": 0.143, "AdvInstruction (\u2191)": 93.68, "OOD detection (\u2191)": 0.398, "OOD generalization (\u2191)": 0.883}, {"Model": "Wizardlm-13b", "AdvGlue RS (\u2191)": 0.152, "AdvInstruction (\u2191)": 95.02, "OOD detection (\u2191)": 0.643, "OOD generalization (\u2191)": 0.871}, {"Model": "ERNIE", "AdvGlue RS (\u2191)": 0.408, "AdvInstruction (\u2191)": 90.99, "OOD detection (\u2191)": 0.548, "OOD generalization (\u2191)": 0.795}, {"Model": "ChatGPT", "AdvGlue RS (\u2191)": 0.326, "AdvInstruction (\u2191)": 97.42, "OOD detection (\u2191)": 0.697, "OOD generalization (\u2191)": 0.867}, {"Model": "GPT-4", "AdvGlue RS (\u2191)": 0.591, "AdvInstruction (\u2191)": 96.36, "OOD detection (\u2191)": 0.805, "OOD generalization (\u2191)": 0.923}, {"Model": "Llama2-70b", "AdvGlue RS (\u2191)": 0.471, "AdvInstruction (\u2191)": 97.64, "OOD detection (\u2191)": 0.461, "OOD generalization (\u2191)": 0.873}, {"Model": "Mistral-7b", "AdvGlue RS (\u2191)": 0.331, "AdvInstruction (\u2191)": 95.76, "OOD detection (\u2191)": 0.407, "OOD generalization (\u2191)": 0.822}, {"Model": "PaLM 2", "AdvGlue RS (\u2191)": 0.607, "AdvInstruction (\u2191)": 95.41, "OOD detection (\u2191)": 0.104, "OOD generalization (\u2191)": 0.822}]
ethics=[{"Model": " GPT-4", "Social Chemistry 101 Acc  (\u2191)": 0.674, "ETHICS Acc  (\u2191)": 0.674, "MoralChoice Acc  (\u2191)": 1.0, "MoralChoice RtA  (\u2191)": 0.669, "Emotional Acc  (\u2191)": 0.945}, {"Model": "PaLM 2", "Social Chemistry 101 Acc  (\u2191)": 0.67, "ETHICS Acc  (\u2191)": 0.602, "MoralChoice Acc  (\u2191)": 0.993, "MoralChoice RtA  (\u2191)": 0.429, "Emotional Acc  (\u2191)": 0.935}, {"Model": "chatgpt", "Social Chemistry 101 Acc  (\u2191)": 0.654, "ETHICS Acc  (\u2191)": 0.668, "MoralChoice Acc  (\u2191)": 1.0, "MoralChoice RtA  (\u2191)": 0.682, "Emotional Acc  (\u2191)": 0.915}, {"Model": "ernie", "Social Chemistry 101 Acc  (\u2191)": 0.651, "ETHICS Acc  (\u2191)": 0.601, "MoralChoice Acc  (\u2191)": 0.993, "MoralChoice RtA  (\u2191)": 0.953, "Emotional Acc  (\u2191)": 0.875}, {"Model": "llama2-70b", "Social Chemistry 101 Acc  (\u2191)": 0.653, "ETHICS Acc  (\u2191)": 0.598, "MoralChoice Acc  (\u2191)": 0.991, "MoralChoice RtA  (\u2191)": 0.999, "Emotional Acc  (\u2191)": 0.875}, {"Model": "wizardlm-13b", "Social Chemistry 101 Acc  (\u2191)": 0.652, "ETHICS Acc  (\u2191)": 0.655, "MoralChoice Acc  (\u2191)": 0.991, "MoralChoice RtA  (\u2191)": 0.85, "Emotional Acc  (\u2191)": 0.81}, {"Model": "Mistral-7b", "Social Chemistry 101 Acc  (\u2191)": 0.647, "ETHICS Acc  (\u2191)": 0.66, "MoralChoice Acc  (\u2191)": 0.987, "MoralChoice RtA  (\u2191)": 0.86, "Emotional Acc  (\u2191)": 0.81}, {"Model": "chatglm2", "Social Chemistry 101 Acc  (\u2191)": 0.588, "ETHICS Acc  (\u2191)": 0.613, "MoralChoice Acc  (\u2191)": 0.942, "MoralChoice RtA  (\u2191)": 0.651, "Emotional Acc  (\u2191)": 0.765}, {"Model": "vicuna-13b", "Social Chemistry 101 Acc  (\u2191)": 0.518, "ETHICS Acc  (\u2191)": 0.633, "MoralChoice Acc  (\u2191)": 0.905, "MoralChoice RtA  (\u2191)": 0.99, "Emotional Acc  (\u2191)": 0.75}, {"Model": "llama2-13b", "Social Chemistry 101 Acc  (\u2191)": 0.619, "ETHICS Acc  (\u2191)": 0.614, "MoralChoice Acc  (\u2191)": 0.962, "MoralChoice RtA  (\u2191)": 0.999, "Emotional Acc  (\u2191)": 0.735}, {"Model": "vicuna-33b", "Social Chemistry 101 Acc  (\u2191)": 0.668, "ETHICS Acc  (\u2191)": 0.643, "MoralChoice Acc  (\u2191)": 0.985, "MoralChoice RtA  (\u2191)": 0.938, "Emotional Acc  (\u2191)": 0.725}, {"Model": "baichuan-13b", "Social Chemistry 101 Acc  (\u2191)": 0.571, "ETHICS Acc  (\u2191)": 0.592, "MoralChoice Acc  (\u2191)": 0.789, "MoralChoice RtA  (\u2191)": 0.622, "Emotional Acc  (\u2191)": 0.705}, {"Model": "llama2-7b", "Social Chemistry 101 Acc  (\u2191)": 0.609, "ETHICS Acc  (\u2191)": 0.657, "MoralChoice Acc  (\u2191)": 0.92, "MoralChoice RtA  (\u2191)": 0.999, "Emotional Acc  (\u2191)": 0.63}, {"Model": "vicuna-7b", "Social Chemistry 101 Acc  (\u2191)": 0.594, "ETHICS Acc  (\u2191)": 0.609, "MoralChoice Acc  (\u2191)": 0.594, "MoralChoice RtA  (\u2191)": 0.944, "Emotional Acc  (\u2191)": 0.485}, {"Model": "koala-13b", "Social Chemistry 101 Acc  (\u2191)": 0.546, "ETHICS Acc  (\u2191)": 0.465, "MoralChoice Acc  (\u2191)": 0.924, "MoralChoice RtA  (\u2191)": 0.872, "Emotional Acc  (\u2191)": 0.34}, {"Model": "oasst-12b", "Social Chemistry 101 Acc  (\u2191)": 0.539, "ETHICS Acc  (\u2191)": 0.492, "MoralChoice Acc  (\u2191)": 0.505, "MoralChoice RtA  (\u2191)": 0.631, "Emotional Acc  (\u2191)": 0.105}]
fairness=[{"Model": "baichuan-13b", "Stereotype Recognition  (\u2191)": 0.388, "Overall Agreement Rate  (\u2193)": 0.279, "Stereotype Query Test (\u2191)": 0.063, "Disparagement Sex (\u2191)": 0.183, "Disparagement Race (\u2191)": 0.001, "Prefereence RtA (\u2191)": 0.021}, {"Model": "chatglm2", "Stereotype Recognition  (\u2191)": 0.391, "Overall Agreement Rate  (\u2193)": 0.594, "Stereotype Query Test (\u2191)": 0.989, "Disparagement Sex (\u2191)": 0.037, "Disparagement Race (\u2191)": 0.0, "Prefereence RtA (\u2191)": 0.579}, {"Model": "chatgpt", "Stereotype Recognition  (\u2191)": 0.608, "Overall Agreement Rate  (\u2193)": 0.12, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.001, "Disparagement Race (\u2191)": 0.136, "Prefereence RtA (\u2191)": 0.425}, {"Model": "ernie", "Stereotype Recognition  (\u2191)": 0.457, "Overall Agreement Rate  (\u2193)": 0.018, "Stereotype Query Test (\u2191)": 0.926, "Disparagement Sex (\u2191)": 0.0, "Disparagement Race (\u2191)": 0.002, "Prefereence RtA (\u2191)": 0.904}, {"Model": "gpt-4", "Stereotype Recognition  (\u2191)": 0.656, "Overall Agreement Rate  (\u2193)": 0.018, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.006, "Disparagement Race (\u2191)": 0.173, "Prefereence RtA (\u2191)": 0.525}, {"Model": "koala-13b", "Stereotype Recognition  (\u2191)": 0.386, "Overall Agreement Rate  (\u2193)": 0.402, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.171, "Disparagement Race (\u2191)": 0.036, "Prefereence RtA (\u2191)": 0.354}, {"Model": "llama2-13b", "Stereotype Recognition  (\u2191)": 0.429, "Overall Agreement Rate  (\u2193)": 0.005, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.0, "Disparagement Race (\u2191)": 0.095, "Prefereence RtA (\u2191)": 0.458}, {"Model": "llama2-70b", "Stereotype Recognition  (\u2191)": 0.616, "Overall Agreement Rate  (\u2193)": 0.084, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.006, "Disparagement Race (\u2191)": 0.01, "Prefereence RtA (\u2191)": 0.513}, {"Model": "llama2-7b", "Stereotype Recognition  (\u2191)": 0.405, "Overall Agreement Rate  (\u2193)": 0.027, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.103, "Disparagement Race (\u2191)": 0.0, "Prefereence RtA (\u2191)": 0.575}, {"Model": "oasst-12b", "Stereotype Recognition  (\u2191)": 0.327, "Overall Agreement Rate  (\u2193)": 0.722, "Stereotype Query Test (\u2191)": 0.958, "Disparagement Sex (\u2191)": 0.64, "Disparagement Race (\u2191)": 0.98, "Prefereence RtA (\u2191)": 0.4}, {"Model": "vicuna-13b", "Stereotype Recognition  (\u2191)": 0.404, "Overall Agreement Rate  (\u2193)": 0.095, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.002, "Disparagement Race (\u2191)": 0.873, "Prefereence RtA (\u2191)": 0.517}, {"Model": "vicuna-33b", "Stereotype Recognition  (\u2191)": 0.505, "Overall Agreement Rate  (\u2193)": 0.399, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.006, "Disparagement Race (\u2191)": 0.793, "Prefereence RtA (\u2191)": 0.413}, {"Model": "vicuna-7b", "Stereotype Recognition  (\u2191)": 0.409, "Overall Agreement Rate  (\u2193)": 0.265, "Stereotype Query Test (\u2191)": 0.937, "Disparagement Sex (\u2191)": 0.431, "Disparagement Race (\u2191)": 0.352, "Prefereence RtA (\u2191)": 0.408}, {"Model": "wizardlm-13b", "Stereotype Recognition  (\u2191)": 0.459, "Overall Agreement Rate  (\u2193)": 0.201, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.017, "Disparagement Race (\u2191)": 0.486, "Prefereence RtA (\u2191)": 0.479}, {"Model": "Mistral-7b", "Stereotype Recognition  (\u2191)": 0.473, "Overall Agreement Rate  (\u2193)": 0.086, "Stereotype Query Test (\u2191)": 0.979, "Disparagement Sex (\u2191)": 0.325, "Disparagement Race (\u2191)": 0.749, "Prefereence RtA (\u2191)": 0.442}, {"Model": "PaLM2", "Stereotype Recognition  (\u2191)": 0.634, "Overall Agreement Rate  (\u2193)": 0.075, "Stereotype Query Test (\u2191)": 0.947, "Disparagement Sex (\u2191)": 0.33, "Disparagement Race (\u2191)": 0.0, "Prefereence RtA (\u2191)": 0.365}]
truthfulness_V2=[{"Model": "Table 7. Internal (\u2191) ", "ChatGPT": 0.288, "GPT-4": 0.417, "ChatGLM2": 0.127, "Vicuna-33b": 0.261, "Vicuna-13b": 0.18, "Vicuna-7b": 0.132, "Llama2-70b": 0.313, "Llama2-13b": 0.235, "Llama2-7b": 0.203, "Wizardlm-13b": 0.19, "Koala-13b": 0.145, "Baichuan-13b": 0.17, "Oasst-12b": 0.101, "ERNIE": 0.255, "Mistral-7b": 0.341, "PaLM2": 0.284}, {"Model": "Table 7. Internal (\u2191) Rank", "ChatGPT": 4.0, "GPT-4": 1.0, "ChatGLM2": 15.0, "Vicuna-33b": 6.0, "Vicuna-13b": 11.0, "Vicuna-7b": 14.0, "Llama2-70b": 3.0, "Llama2-13b": 8.0, "Llama2-7b": 9.0, "Wizardlm-13b": 10.0, "Koala-13b": 13.0, "Baichuan-13b": 12.0, "Oasst-12b": 16.0, "ERNIE": 7.0, "Mistral-7b": 2.0, "PaLM2": 5.0}, {"Model": "Table 9. External  (\u2191)", "ChatGPT": 0.726, "GPT-4": 0.793, "ChatGLM2": 0.542, "Vicuna-33b": 0.726, "Vicuna-13b": 0.623, "Vicuna-7b": 0.581, "Llama2-70b": 0.721, "Llama2-13b": 0.722, "Llama2-7b": 0.638, "Wizardlm-13b": 0.574, "Koala-13b": 0.553, "Baichuan-13b": 0.622, "Oasst-12b": 0.534, "ERNIE": 0.689, "Mistral-7b": 0.687, "PaLM2": 0.532}, {"Model": "Table 9. External  (\u2191) Rank", "ChatGPT": 2.0, "GPT-4": 1.0, "ChatGLM2": 14.0, "Vicuna-33b": 2.0, "Vicuna-13b": 9.0, "Vicuna-7b": 11.0, "Llama2-70b": 5.0, "Llama2-13b": 4.0, "Llama2-7b": 8.0, "Wizardlm-13b": 12.0, "Koala-13b": 13.0, "Baichuan-13b": 10.0, "Oasst-12b": 15.0, "ERNIE": 6.0, "Mistral-7b": 7.0, "PaLM2": 16.0}, {"Model": "Table 10. Hallucination  (\u2191)", "ChatGPT": 0.529, "GPT-4": 0.516, "ChatGLM2": 0.542, "Vicuna-33b": 0.423, "Vicuna-13b": 0.403, "Vicuna-7b": 0.347, "Llama2-70b": 0.402, "Llama2-13b": 0.404, "Llama2-7b": 0.396, "Wizardlm-13b": 0.356, "Koala-13b": 0.451, "Baichuan-13b": 0.306, "Oasst-12b": 0.418, "ERNIE": 0.515, "Mistral-7b": 0.458, "PaLM2": 0.379}, {"Model": "Table 10. Hallucination  (\u2191) Rank", "ChatGPT": 2.0, "GPT-4": 3.0, "ChatGLM2": 1.0, "Vicuna-33b": 7.0, "Vicuna-13b": 9.0, "Vicuna-7b": 15.0, "Llama2-70b": 10.0, "Llama2-13b": 8.0, "Llama2-7b": 12.0, "Wizardlm-13b": 14.0, "Koala-13b": 6.0, "Baichuan-13b": 16.0, "Oasst-12b": 7.0, "ERNIE": 4.0, "Mistral-7b": 5.0, "PaLM2": 13.0}, {"Model": "Table 13. Persona Sycophancy  (\u2193)", "ChatGPT": 0.039, "GPT-4": 0.029, "ChatGLM2": 0.036, "Vicuna-33b": 0.038, "Vicuna-13b": 0.036, "Vicuna-7b": 0.03, "Llama2-70b": 0.043, "Llama2-13b": 0.032, "Llama2-7b": 0.035, "Wizardlm-13b": 0.025, "Koala-13b": 0.04, "Baichuan-13b": 0.032, "Oasst-12b": 0.031, "ERNIE": 0.019, "Mistral-7b": 0.035, "PaLM2": 0.028}, {"Model": "Table 13. Persona Rank", "ChatGPT": 3.0, "GPT-4": 13.0, "ChatGLM2": 5.0, "Vicuna-33b": 4.0, "Vicuna-13b": 5.0, "Vicuna-7b": 12.0, "Llama2-70b": 1.0, "Llama2-13b": 9.0, "Llama2-7b": 7.0, "Wizardlm-13b": 15.0, "Koala-13b": 2.0, "Baichuan-13b": 9.0, "Oasst-12b": 11.0, "ERNIE": 16.0, "Mistral-7b": 7.0, "PaLM2": 4.0}, {"Model": "Table 13. Preference Sycophancy  (\u2193)", "ChatGPT": 0.257, "GPT-4": 0.296, "ChatGLM2": 0.432, "Vicuna-33b": 0.458, "Vicuna-13b": 0.375, "Vicuna-7b": 0.395, "Llama2-70b": 0.468, "Llama2-13b": 0.571, "Llama2-7b": 0.587, "Wizardlm-13b": 0.385, "Koala-13b": 0.5, "Baichuan-13b": 0.286, "Oasst-12b": 0.436, "ERNIE": 0.312, "Mistral-7b": 0.293, "PaLM2": 0.581}, {"Model": "Table 13. Preference Rank", "ChatGPT": 1.0, "GPT-4": 4.0, "ChatGLM2": 9.0, "Vicuna-33b": 11.0, "Vicuna-13b": 6.0, "Vicuna-7b": 8.0, "Llama2-70b": 12.0, "Llama2-13b": 14.0, "Llama2-7b": 16.0, "Wizardlm-13b": 7.0, "Koala-13b": 13.0, "Baichuan-13b": 2.0, "Oasst-12b": 10.0, "ERNIE": 5.0, "Mistral-7b": 3.0, "PaLM2": 15.0}, {"Model": "Table 15. Adv Factuality  (\u2191)", "ChatGPT": 0.708, "GPT-4": 0.813, "ChatGLM2": 0.349, "Vicuna-33b": 0.699, "Vicuna-13b": 0.665, "Vicuna-7b": 0.469, "Llama2-70b": 0.794, "Llama2-13b": 0.78, "Llama2-7b": 0.718, "Wizardlm-13b": 0.794, "Koala-13b": 0.435, "Baichuan-13b": 0.44, "Oasst-12b": 0.221, "ERNIE": 0.407, "Mistral-7b": 0.426, "PaLM2": 0.273}, {"Model": "Table 15. Adv Factuality  (\u2191) Rank", "ChatGPT": 6.0, "GPT-4": 1.0, "ChatGLM2": 14.0, "Vicuna-33b": 7.0, "Vicuna-13b": 8.0, "Vicuna-7b": 9.0, "Llama2-70b": 2.0, "Llama2-13b": 4.0, "Llama2-7b": 5.0, "Wizardlm-13b": 2.0, "Koala-13b": 11.0, "Baichuan-13b": 10.0, "Oasst-12b": 16.0, "ERNIE": 13.0, "Mistral-7b": 12.0, "PaLM2": 15.0}]
safety=[{"Model": "Llama2-13b", "Jailbreak (\u2191)": 0.959, "Toxicity (\u2193)": 0.205, "Misuse (\u2191)": 0.963, "Exaggerated Safety (\u2193)": 0.55}, {"Model": "Llama2-70b", "Jailbreak (\u2191)": 0.974, "Toxicity (\u2193)": 0.248, "Misuse (\u2191)": 0.956, "Exaggerated Safety (\u2193)": 0.315}, {"Model": "Llama2-7b", "Jailbreak (\u2191)": 0.945, "Toxicity (\u2193)": 0.191, "Misuse (\u2191)": 0.943, "Exaggerated Safety (\u2193)": 0.49}, {"Model": "GPT-4", "Jailbreak (\u2191)": 0.914, "Toxicity (\u2193)": 0.386, "Misuse (\u2191)": 0.924, "Exaggerated Safety (\u2193)": 0.085}, {"Model": "ChatGPT", "Jailbreak (\u2191)": 0.898, "Toxicity (\u2193)": 0.352, "Misuse (\u2191)": 0.91, "Exaggerated Safety (\u2193)": 0.15}, {"Model": "ERNIE", "Jailbreak (\u2191)": 0.949, "Toxicity (\u2193)": 0.072, "Misuse (\u2191)": 0.899, "Exaggerated Safety (\u2193)": 0.385}, {"Model": "Wizardlm-13b", "Jailbreak (\u2191)": 0.865, "Toxicity (\u2193)": 0.183, "Misuse (\u2191)": 0.883, "Exaggerated Safety (\u2193)": 0.06}, {"Model": "Vicuna-13b", "Jailbreak (\u2191)": 0.781, "Toxicity (\u2193)": 0.374, "Misuse (\u2191)": 0.848, "Exaggerated Safety (\u2193)": 0.095}, {"Model": "ChatGLM2", "Jailbreak (\u2191)": 0.845, "Toxicity (\u2193)": 0.141, "Misuse (\u2191)": 0.819, "Exaggerated Safety (\u2193)": 0.15}, {"Model": "Koala-13b", "Jailbreak (\u2191)": 0.691, "Toxicity (\u2193)": 0.237, "Misuse (\u2191)": 0.738, "Exaggerated Safety (\u2193)": 0.045}, {"Model": "Vicuna-33b", "Jailbreak (\u2191)": 0.585, "Toxicity (\u2193)": 0.294, "Misuse (\u2191)": 0.735, "Exaggerated Safety (\u2193)": 0.035}, {"Model": "Mistral-7b", "Jailbreak (\u2191)": 0.59, "Toxicity (\u2193)": 0.262, "Misuse (\u2191)": 0.709, "Exaggerated Safety (\u2193)": 0.46}, {"Model": "Oasst-12b", "Jailbreak (\u2191)": 0.69, "Toxicity (\u2193)": 0.154, "Misuse (\u2191)": 0.583, "Exaggerated Safety (\u2193)": 0.05}, {"Model": "Vicuna-7b", "Jailbreak (\u2191)": 0.596, "Toxicity (\u2193)": 0.213, "Misuse (\u2191)": 0.565, "Exaggerated Safety (\u2193)": 0.09}, {"Model": "PaLM 2", "Jailbreak (\u2191)": 0.486, "Toxicity (\u2193)": 0.317, "Misuse (\u2191)": 0.473, "Exaggerated Safety (\u2193)": 0.377}, {"Model": "Baichuan-13b", "Jailbreak (\u2191)": 0.25, "Toxicity (\u2193)": 0.112, "Misuse (\u2191)": 0.114, "Exaggerated Safety (\u2193)": 0.19}]
