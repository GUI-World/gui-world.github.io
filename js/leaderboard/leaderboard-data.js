score=[{"Model": "LLaVA","COCO": 0.247,"C.C.": 0.227,"Diffusion": 0.06,"Graphics": 0.242,"Math": 0.093,"Text": 0.245,"WIT": 0.109,"Chart": 0.237,"VisIT": 0.177,"CC-3M": 0.071,"Average Score": 0.171},{"Model": "CogVLM","COCO": 0.107,"C.C.": -0.048,"Diffusion": 0.049,"Graphics": -0.158,"Math": 0.065,"Text": 0.097,"WIT": -0.131,"Chart": -0.135,"VisIT": 0.278,"CC-3M": 0.157,"Average Score": 0.028},{"Model": "Gemini","COCO": 0.262,"C.C.": 0.408,"Diffusion": null,"Graphics": 0.4,"Math": 0.228,"Text": 0.222,"WIT": 0.418,"Chart": 0.343,"VisIT": 0.336,"CC-3M": 0.374,"Average Score": 0.332},{"Model": "GPT-4V","COCO": 0.454,"C.C.": 0.507,"Diffusion": 0.458,"Graphics": 0.645,"Math": 0.606,"Text": 0.624,"WIT": 0.579,"Chart": 0.645,"VisIT": 0.62,"CC-3M": 0.431,"Average Score": 0.557}]
pairwtie=[{"Model": "LLaVA","COCO": 0.273,"C.C.": 0.478,"Diffusion": 0.286,"Graphics": 0.273,"Math": 0.657,"Text": 0.51,"WIT": 0.369,"Chart": 0.383,"VisIT": 0.456,"CC-3M": 0.484,"Average Score": 0.417},{"Model": "CogVLM","COCO": 0.548,"C.C.": 0.409,"Diffusion": 0.562,"Graphics": 0.613,"Math": 0.412,"Text": 0.25,"WIT": 0.273,"Chart": 0.262,"VisIT": 0.324,"CC-3M": 0.433,"Average Score": 0.409},{"Model": "Gemini","COCO": 0.616,"C.C.": 0.787,"Diffusion": null,"Graphics": 0.65,"Math": 0.436,"Text": 0.664,"WIT": 0.605,"Chart": 0.5,"VisIT": 0.66,"CC-3M": 0.56,"Average Score": 0.609},{"Model": "GPT-4V","COCO": 0.696,"C.C.": 0.824,"Diffusion": 0.847,"Graphics": 0.639,"Math": 0.564,"Text": 0.673,"WIT": 0.679,"Chart": 0.657,"VisIT": 0.64,"CC-3M": 0.612,"Average Score": 0.683}]
PairwoTie=[{"Model": "LLaVA","COCO": 0.327,"C.C.": 0.537,"Diffusion": 0.302,"Graphics": 0.3,"Math": 0.726,"Text": 0.684,"WIT": 0.6,"Chart": 0.61,"VisIT": 0.648,"CC-3M": 0.583,"Average Score": 0.532},{"Model": "CogVLM","COCO": 0.654,"C.C.": 0.45,"Diffusion": 0.643,"Graphics": 0.704,"Math": 0.481,"Text": 0.292,"WIT": 0.5,"Chart": 0.423,"VisIT": 0.5,"CC-3M": 0.591,"Average Score": 0.524},{"Model": "Gemini","COCO": 0.717,"C.C.": 0.84,"Diffusion": null,"Graphics": 0.77,"Math": 0.678,"Text": 0.793,"WIT": 0.688,"Chart": 0.658,"VisIT": 0.711,"CC-3M": 0.652,"Average Score": 0.723},{"Model": "GPT-4V","COCO": 0.804,"C.C.": 0.87,"Diffusion": 0.922,"Graphics": 0.807,"Math": 0.801,"Text": 0.805,"WIT": 0.734,"Chart": 0.849,"VisIT": 0.761,"CC-3M": 0.703,"Average Score": 0.806}]
batch=[{"Model": "LLaVA","COCO": 0.577,"C.C.": 0.492,"Diffusion": 0.562,"Graphics": 0.535,"Math": 0.598,"Text": 0.65,"WIT": 0.616,"Chart": 0.644,"VisIT": 0.62,"CC-3M": 0.563,"Average Score": 0.586},{"Model": "Gemini","COCO": 0.287,"C.C.": 0.299,"Diffusion": null,"Graphics": 0.473,"Math": 0.462,"Text": 0.43,"WIT": 0.344,"Chart": 0.52,"VisIT": 0.426,"CC-3M": 0.357,"Average Score": 0.4},{"Model": "GPT-4V","COCO": 0.318,"C.C.": 0.353,"Diffusion": 0.07,"Graphics": 0.385,"Math": 0.348,"Text": 0.319,"WIT": 0.29,"Chart": 0.347,"VisIT": 0.3,"CC-3M": 0.402,"Average Score": 0.313}]
truthfulness2=[{"Model": "ChatGPT", "Internal (\u2191)": 0.288, "Externa(\u2191) ": 0.726, "Hallucinatio(\u2191)": 0.529, "Persona Sycophanc(\u2193)": 0.039, "Preference Sycophanc(\u2193)": 0.257, "Adv Factualit(\u2191)": 0.708}, {"Model": "GPT-4", "Internal (\u2191) ": 0.417, "Externa(\u2191)": 0.793, "Hallucinatio(\u2191)": 0.516, "Persona Sycophanc(\u2193)": 0.029, "Preference Sycophanc(\u2193)": 0.296, "Adv Factualit(\u2191)": 0.813}, {"Model": "ChatGLM2", "Internal (\u2191) ": 0.127, "Externa(\u2191)": 0.542, "Hallucinatio(\u2191)": 0.542, "Persona Sycophanc(\u2193)": 0.036, "Preference Sycophanc(\u2193)": 0.432, "Adv Factualit(\u2191)": 0.349}, {"Model": "Vicuna-33b", "Internal (\u2191) ": 0.261, "Externa(\u2191)": 0.726, "Hallucinatio(\u2191)": 0.423, "Persona Sycophanc(\u2193)": 0.038, "Preference Sycophanc(\u2193)": 0.458, "Adv Factualit(\u2191)": 0.699}, {"Model": "Vicuna-13b", "Internal (\u2191) ": 0.18, "Externa(\u2191)": 0.623, "Hallucinatio(\u2191)": 0.403, "Persona Sycophanc(\u2193)": 0.036, "Preference Sycophanc(\u2193)": 0.375, "Adv Factualit(\u2191)": 0.665}, {"Model": "Vicuna-7b", "Internal (\u2191) ": 0.132, "Externa(\u2191)": 0.581, "Hallucinatio(\u2191)": 0.347, "Persona Sycophanc(\u2193)": 0.03, "Preference Sycophanc(\u2193)": 0.395, "Adv Factualit(\u2191)": 0.469}, {"Model": "Llama2-70b", "Internal (\u2191) ": 0.313, "Externa(\u2191)": 0.721, "Hallucinatio(\u2191)": 0.402, "Persona Sycophanc(\u2193)": 0.043, "Preference Sycophanc(\u2193)": 0.468, "Adv Factualit(\u2191)": 0.794}, {"Model": "Llama2-13b", "Internal (\u2191) ": 0.235, "Externa(\u2191)": 0.722, "Hallucinatio(\u2191)": 0.404, "Persona Sycophanc(\u2193)": 0.032, "Preference Sycophanc(\u2193)": 0.571, "Adv Factualit(\u2191)": 0.78}, {"Model": "Llama2-7b", "Internal (\u2191) ": 0.203, "Externa(\u2191)": 0.638, "Hallucinatio(\u2191)": 0.396, "Persona Sycophanc(\u2193)": 0.035, "Preference Sycophanc(\u2193)": 0.587, "Adv Factualit(\u2191)": 0.718}, {"Model": "Wizardlm-13b", "Internal (\u2191) ": 0.19, "Externa(\u2191)": 0.574, "Hallucinatio(\u2191)": 0.356, "Persona Sycophanc(\u2193)": 0.025, "Preference Sycophanc(\u2193)": 0.385, "Adv Factualit(\u2191)": 0.794}, {"Model": "Koala-13b", "Internal (\u2191) ": 0.145, "Externa(\u2191)": 0.553, "Hallucinatio(\u2191)": 0.451, "Persona Sycophanc(\u2193)": 0.04, "Preference Sycophanc(\u2193)": 0.5, "Adv Factualit(\u2191)": 0.435}, {"Model": "Baichuan-13b", "Internal (\u2191) ": 0.17, "Externa(\u2191)": 0.622, "Hallucinatio(\u2191)": 0.306, "Persona Sycophanc(\u2193)": 0.032, "Preference Sycophanc(\u2193)": 0.286, "Adv Factualit(\u2191)": 0.44}, {"Model": "Oasst-12b", "Internal (\u2191) ": 0.101, "Externa(\u2191)": 0.534, "Hallucinatio(\u2191)": 0.418, "Persona Sycophanc(\u2193)": 0.031, "Preference Sycophanc(\u2193)": 0.436, "Adv Factualit(\u2191)": 0.221}, {"Model": "ERNIE", "Internal (\u2191) ": 0.255, "Externa(\u2191)": 0.689, "Hallucinatio(\u2191)": 0.515, "Persona Sycophanc(\u2193)": 0.019, "Preference Sycophanc(\u2193)": 0.312, "Adv Factualit(\u2191)": 0.407}, {"Model": "Mistral-7b", "Internal (\u2191) ": 0.341, "Externa(\u2191)": 0.687, "Hallucinatio(\u2191)": 0.458, "Persona Sycophanc(\u2193)": 0.035, "Preference Sycophanc(\u2193)": 0.293, "Adv Factualit(\u2191)": 0.426}, {"Model": "PaLM2", "Internal (\u2191) ": 0.284, "Externa(\u2191)": 0.532, "Hallucinatio(\u2191)": 0.379, "Persona Sycophanc(\u2193)": 0.028, "Preference Sycophanc(\u2193)": 0.581, "Adv Factualit(\u2191)": 0.273}]
truthfulness=[{"Model": "ChatGPT", "Internal (\u2191)": 0.288, "Externa(\u2191)": 0.726, "Hallucinatio(\u2191)": 0.529, "Persona Sycophanc(\u2193)": 0.039, "Preference Sycophanc(\u2193)": 0.257, "Adv Factualit(\u2191)": 0.708}, {"Model": "GPT-4", "Internal (\u2191) ": 0.417, "Externa(\u2191)": 0.793, "Hallucinatio(\u2191)": 0.516, "Persona Sycophanc(\u2193)": 0.029, "Preference Sycophanc(\u2193)": 0.296, "Adv Factualit(\u2191)": 0.813}, {"Model": "ChatGLM2", "Internal (\u2191) ": 0.127, "Externa(\u2191)": 0.542, "Hallucinatio(\u2191)": 0.542, "Persona Sycophanc(\u2193)": 0.036, "Preference Sycophanc(\u2193)": 0.432, "Adv Factualit(\u2191)": 0.349}, {"Model": "Vicuna-33b", "Internal (\u2191) ": 0.261, "Externa(\u2191)": 0.726, "Hallucinatio(\u2191)": 0.423, "Persona Sycophanc(\u2193)": 0.038, "Preference Sycophanc(\u2193)": 0.458, "Adv Factualit(\u2191)": 0.699}, {"Model": "Vicuna-13b", "Internal (\u2191) ": 0.18, "Externa(\u2191)": 0.623, "Hallucinatio(\u2191)": 0.403, "Persona Sycophanc(\u2193)": 0.036, "Preference Sycophanc(\u2193)": 0.375, "Adv Factualit(\u2191)": 0.665}, {"Model": "Vicuna-7b", "Internal (\u2191) ": 0.132, "Externa(\u2191)": 0.581, "Hallucinatio(\u2191)": 0.347, "Persona Sycophanc(\u2193)": 0.03, "Preference Sycophanc(\u2193)": 0.395, "Adv Factualit(\u2191)": 0.469}, {"Model": "Llama2-70b", "Internal (\u2191) ": 0.313, "Externa(\u2191)": 0.721, "Hallucinatio(\u2191)": 0.402, "Persona Sycophanc(\u2193)": 0.043, "Preference Sycophanc(\u2193)": 0.468, "Adv Factualit(\u2191)": 0.794}, {"Model": "Llama2-13b", "Internal (\u2191) ": 0.235, "Externa(\u2191)": 0.722, "Hallucinatio(\u2191)": 0.404, "Persona Sycophanc(\u2193)": 0.032, "Preference Sycophanc(\u2193)": 0.571, "Adv Factualit(\u2191)": 0.78}, {"Model": "Llama2-7b", "Internal (\u2191) ": 0.203, "Externa(\u2191)": 0.638, "Hallucinatio(\u2191)": 0.396, "Persona Sycophanc(\u2193)": 0.035, "Preference Sycophanc(\u2193)": 0.587, "Adv Factualit(\u2191)": 0.718}, {"Model": "Wizardlm-13b", "Internal (\u2191) ": 0.19, "Externa(\u2191)": 0.574, "Hallucinatio(\u2191)": 0.356, "Persona Sycophanc(\u2193)": 0.025, "Preference Sycophanc(\u2193)": 0.385, "Adv Factualit(\u2191)": 0.794}, {"Model": "Koala-13b", "Internal (\u2191) ": 0.145, "Externa(\u2191)": 0.553, "Hallucinatio(\u2191)": 0.451, "Persona Sycophanc(\u2193)": 0.04, "Preference Sycophanc(\u2193)": 0.5, "Adv Factualit(\u2191)": 0.435}, {"Model": "Baichuan-13b", "Internal (\u2191) ": 0.17, "Externa(\u2191)": 0.622, "Hallucinatio(\u2191)": 0.306, "Persona Sycophanc(\u2193)": 0.032, "Preference Sycophanc(\u2193)": 0.286, "Adv Factualit(\u2191)": 0.44}, {"Model": "Oasst-12b", "Internal (\u2191) ": 0.101, "Externa(\u2191)": 0.534, "Hallucinatio(\u2191)": 0.418, "Persona Sycophanc(\u2193)": 0.031, "Preference Sycophanc(\u2193)": 0.436, "Adv Factualit(\u2191)": 0.221}, {"Model": "ERNIE", "Internal (\u2191) ": 0.255, "Externa(\u2191)": 0.689, "Hallucinatio(\u2191)": 0.515, "Persona Sycophanc(\u2193)": 0.019, "Preference Sycophanc(\u2193)": 0.312, "Adv Factualit(\u2191)": 0.407}, {"Model": "Mistral-7b", "Internal (\u2191) ": 0.341, "Externa(\u2191)": 0.687, "Hallucinatio(\u2191)": 0.458, "Persona Sycophanc(\u2193)": 0.035, "Preference Sycophanc(\u2193)": 0.293, "Adv Factualit(\u2191)": 0.426}, {"Model": "PaLM2", "Internal (\u2191) ": 0.284, "Externa(\u2191)": 0.532, "Hallucinatio(\u2191)": 0.379, "Persona Sycophanc(\u2193)": 0.028, "Preference Sycophanc(\u2193)": 0.581, "Adv Factualit(\u2191)": 0.273}]
robustness=[{"Model": "Baichuan-13b", "AdvGlue RS (\u2191)": 0.363, "AdvInstruction (\u2191)": 93.99, "OOD detection (\u2191)": 0.004, "OOD generalization (\u2191)": 0.539}, {"Model": "ChatGLM2", "AdvGlue RS (\u2191)": 0.254, "AdvInstruction (\u2191)": 94.37, "OOD detection (\u2191)": 0.627, "OOD generalization (\u2191)": 0.778}, {"Model": "Vicuna-13b", "AdvGlue RS (\u2191)": 0.18, "AdvInstruction (\u2191)": 96.01, "OOD detection (\u2191)": 0.635, "OOD generalization (\u2191)": 0.839}, {"Model": "Vicuna-7b", "AdvGlue RS (\u2191)": 0.072, "AdvInstruction (\u2191)": 88.77, "OOD detection (\u2191)": 0.49, "OOD generalization (\u2191)": 0.753}, {"Model": "Vicuna-33b", "AdvGlue RS (\u2191)": 0.219, "AdvInstruction (\u2191)": 95.97, "OOD detection (\u2191)": 0.685, "OOD generalization (\u2191)": 0.785}, {"Model": "Llama2-7b", "AdvGlue RS (\u2191)": 0.374, "AdvInstruction (\u2191)": 96.68, "OOD detection (\u2191)": 0.465, "OOD generalization (\u2191)": 0.777}, {"Model": "Llama2-13b", "AdvGlue RS (\u2191)": 0.306, "AdvInstruction (\u2191)": 96.48, "OOD detection (\u2191)": 0.432, "OOD generalization (\u2191)": 0.884}, {"Model": "Koala-13b", "AdvGlue RS (\u2191)": 0.116, "AdvInstruction (\u2191)": 94.05, "OOD detection (\u2191)": 0.552, "OOD generalization (\u2191)": 0.584}, {"Model": "Oasst-12b", "AdvGlue RS (\u2191)": 0.143, "AdvInstruction (\u2191)": 93.68, "OOD detection (\u2191)": 0.398, "OOD generalization (\u2191)": 0.883}, {"Model": "Wizardlm-13b", "AdvGlue RS (\u2191)": 0.152, "AdvInstruction (\u2191)": 95.02, "OOD detection (\u2191)": 0.643, "OOD generalization (\u2191)": 0.871}, {"Model": "ERNIE", "AdvGlue RS (\u2191)": 0.408, "AdvInstruction (\u2191)": 90.99, "OOD detection (\u2191)": 0.548, "OOD generalization (\u2191)": 0.795}, {"Model": "ChatGPT", "AdvGlue RS (\u2191)": 0.326, "AdvInstruction (\u2191)": 97.42, "OOD detection (\u2191)": 0.697, "OOD generalization (\u2191)": 0.867}, {"Model": "GPT-4", "AdvGlue RS (\u2191)": 0.591, "AdvInstruction (\u2191)": 96.36, "OOD detection (\u2191)": 0.805, "OOD generalization (\u2191)": 0.923}, {"Model": "Llama2-70b", "AdvGlue RS (\u2191)": 0.471, "AdvInstruction (\u2191)": 97.64, "OOD detection (\u2191)": 0.461, "OOD generalization (\u2191)": 0.873}, {"Model": "Mistral-7b", "AdvGlue RS (\u2191)": 0.331, "AdvInstruction (\u2191)": 95.76, "OOD detection (\u2191)": 0.407, "OOD generalization (\u2191)": 0.822}, {"Model": "PaLM 2", "AdvGlue RS (\u2191)": 0.607, "AdvInstruction (\u2191)": 95.41, "OOD detection (\u2191)": 0.104, "OOD generalization (\u2191)": 0.822}]
ethics=[{"Model": " GPT-4", "Social Chemistry 101 Ac(\u2191)": 0.674, "ETHICS Ac(\u2191)": 0.674, "MoralChoice Ac(\u2191)": 1.0, "MoralChoice Rt(\u2191)": 0.669, "Emotional Ac(\u2191)": 0.945}, {"Model": "PaLM 2", "Social Chemistry 101 Ac(\u2191)": 0.67, "ETHICS Ac(\u2191)": 0.602, "MoralChoice Ac(\u2191)": 0.993, "MoralChoice Rt(\u2191)": 0.429, "Emotional Ac(\u2191)": 0.935}, {"Model": "chatgpt", "Social Chemistry 101 Ac(\u2191)": 0.654, "ETHICS Ac(\u2191)": 0.668, "MoralChoice Ac(\u2191)": 1.0, "MoralChoice Rt(\u2191)": 0.682, "Emotional Ac(\u2191)": 0.915}, {"Model": "ernie", "Social Chemistry 101 Ac(\u2191)": 0.651, "ETHICS Ac(\u2191)": 0.601, "MoralChoice Ac(\u2191)": 0.993, "MoralChoice Rt(\u2191)": 0.953, "Emotional Ac(\u2191)": 0.875}, {"Model": "llama2-70b", "Social Chemistry 101 Ac(\u2191)": 0.653, "ETHICS Ac(\u2191)": 0.598, "MoralChoice Ac(\u2191)": 0.991, "MoralChoice Rt(\u2191)": 0.999, "Emotional Ac(\u2191)": 0.875}, {"Model": "wizardlm-13b", "Social Chemistry 101 Ac(\u2191)": 0.652, "ETHICS Ac(\u2191)": 0.655, "MoralChoice Ac(\u2191)": 0.991, "MoralChoice Rt(\u2191)": 0.85, "Emotional Ac(\u2191)": 0.81}, {"Model": "Mistral-7b", "Social Chemistry 101 Ac(\u2191)": 0.647, "ETHICS Ac(\u2191)": 0.66, "MoralChoice Ac(\u2191)": 0.987, "MoralChoice Rt(\u2191)": 0.86, "Emotional Ac(\u2191)": 0.81}, {"Model": "chatglm2", "Social Chemistry 101 Ac(\u2191)": 0.588, "ETHICS Ac(\u2191)": 0.613, "MoralChoice Ac(\u2191)": 0.942, "MoralChoice Rt(\u2191)": 0.651, "Emotional Ac(\u2191)": 0.765}, {"Model": "vicuna-13b", "Social Chemistry 101 Ac(\u2191)": 0.518, "ETHICS Ac(\u2191)": 0.633, "MoralChoice Ac(\u2191)": 0.905, "MoralChoice Rt(\u2191)": 0.99, "Emotional Ac(\u2191)": 0.75}, {"Model": "llama2-13b", "Social Chemistry 101 Ac(\u2191)": 0.619, "ETHICS Ac(\u2191)": 0.614, "MoralChoice Ac(\u2191)": 0.962, "MoralChoice Rt(\u2191)": 0.999, "Emotional Ac(\u2191)": 0.735}, {"Model": "vicuna-33b", "Social Chemistry 101 Ac(\u2191)": 0.668, "ETHICS Ac(\u2191)": 0.643, "MoralChoice Ac(\u2191)": 0.985, "MoralChoice Rt(\u2191)": 0.938, "Emotional Ac(\u2191)": 0.725}, {"Model": "baichuan-13b", "Social Chemistry 101 Ac(\u2191)": 0.571, "ETHICS Ac(\u2191)": 0.592, "MoralChoice Ac(\u2191)": 0.789, "MoralChoice Rt(\u2191)": 0.622, "Emotional Ac(\u2191)": 0.705}, {"Model": "llama2-7b", "Social Chemistry 101 Ac(\u2191)": 0.609, "ETHICS Ac(\u2191)": 0.657, "MoralChoice Ac(\u2191)": 0.92, "MoralChoice Rt(\u2191)": 0.999, "Emotional Ac(\u2191)": 0.63}, {"Model": "vicuna-7b", "Social Chemistry 101 Ac(\u2191)": 0.594, "ETHICS Ac(\u2191)": 0.609, "MoralChoice Ac(\u2191)": 0.594, "MoralChoice Rt(\u2191)": 0.944, "Emotional Ac(\u2191)": 0.485}, {"Model": "koala-13b", "Social Chemistry 101 Ac(\u2191)": 0.546, "ETHICS Ac(\u2191)": 0.465, "MoralChoice Ac(\u2191)": 0.924, "MoralChoice Rt(\u2191)": 0.872, "Emotional Ac(\u2191)": 0.34}, {"Model": "oasst-12b", "Social Chemistry 101 Ac(\u2191)": 0.539, "ETHICS Ac(\u2191)": 0.492, "MoralChoice Ac(\u2191)": 0.505, "MoralChoice Rt(\u2191)": 0.631, "Emotional Ac(\u2191)": 0.105}]
fairness=[{"Model": "baichuan-13b", "Stereotype Recognitio(\u2191)": 0.388, "Overall Agreement Rat(\u2193)": 0.279, "Stereotype Query Test (\u2191)": 0.063, "Disparagement Sex (\u2191)": 0.183, "Disparagement Race (\u2191)": 0.001, "Prefereence RtA (\u2191)": 0.021}, {"Model": "chatglm2", "Stereotype Recognitio(\u2191)": 0.391, "Overall Agreement Rat(\u2193)": 0.594, "Stereotype Query Test (\u2191)": 0.989, "Disparagement Sex (\u2191)": 0.037, "Disparagement Race (\u2191)": 0.0, "Prefereence RtA (\u2191)": 0.579}, {"Model": "chatgpt", "Stereotype Recognitio(\u2191)": 0.608, "Overall Agreement Rat(\u2193)": 0.12, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.001, "Disparagement Race (\u2191)": 0.136, "Prefereence RtA (\u2191)": 0.425}, {"Model": "ernie", "Stereotype Recognitio(\u2191)": 0.457, "Overall Agreement Rat(\u2193)": 0.018, "Stereotype Query Test (\u2191)": 0.926, "Disparagement Sex (\u2191)": 0.0, "Disparagement Race (\u2191)": 0.002, "Prefereence RtA (\u2191)": 0.904}, {"Model": "gpt-4", "Stereotype Recognitio(\u2191)": 0.656, "Overall Agreement Rat(\u2193)": 0.018, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.006, "Disparagement Race (\u2191)": 0.173, "Prefereence RtA (\u2191)": 0.525}, {"Model": "koala-13b", "Stereotype Recognitio(\u2191)": 0.386, "Overall Agreement Rat(\u2193)": 0.402, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.171, "Disparagement Race (\u2191)": 0.036, "Prefereence RtA (\u2191)": 0.354}, {"Model": "llama2-13b", "Stereotype Recognitio(\u2191)": 0.429, "Overall Agreement Rat(\u2193)": 0.005, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.0, "Disparagement Race (\u2191)": 0.095, "Prefereence RtA (\u2191)": 0.458}, {"Model": "llama2-70b", "Stereotype Recognitio(\u2191)": 0.616, "Overall Agreement Rat(\u2193)": 0.084, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.006, "Disparagement Race (\u2191)": 0.01, "Prefereence RtA (\u2191)": 0.513}, {"Model": "llama2-7b", "Stereotype Recognitio(\u2191)": 0.405, "Overall Agreement Rat(\u2193)": 0.027, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.103, "Disparagement Race (\u2191)": 0.0, "Prefereence RtA (\u2191)": 0.575}, {"Model": "oasst-12b", "Stereotype Recognitio(\u2191)": 0.327, "Overall Agreement Rat(\u2193)": 0.722, "Stereotype Query Test (\u2191)": 0.958, "Disparagement Sex (\u2191)": 0.64, "Disparagement Race (\u2191)": 0.98, "Prefereence RtA (\u2191)": 0.4}, {"Model": "vicuna-13b", "Stereotype Recognitio(\u2191)": 0.404, "Overall Agreement Rat(\u2193)": 0.095, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.002, "Disparagement Race (\u2191)": 0.873, "Prefereence RtA (\u2191)": 0.517}, {"Model": "vicuna-33b", "Stereotype Recognitio(\u2191)": 0.505, "Overall Agreement Rat(\u2193)": 0.399, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.006, "Disparagement Race (\u2191)": 0.793, "Prefereence RtA (\u2191)": 0.413}, {"Model": "vicuna-7b", "Stereotype Recognitio(\u2191)": 0.409, "Overall Agreement Rat(\u2193)": 0.265, "Stereotype Query Test (\u2191)": 0.937, "Disparagement Sex (\u2191)": 0.431, "Disparagement Race (\u2191)": 0.352, "Prefereence RtA (\u2191)": 0.408}, {"Model": "wizardlm-13b", "Stereotype Recognitio(\u2191)": 0.459, "Overall Agreement Rat(\u2193)": 0.201, "Stereotype Query Test (\u2191)": 1.0, "Disparagement Sex (\u2191)": 0.017, "Disparagement Race (\u2191)": 0.486, "Prefereence RtA (\u2191)": 0.479}, {"Model": "Mistral-7b", "Stereotype Recognitio(\u2191)": 0.473, "Overall Agreement Rat(\u2193)": 0.086, "Stereotype Query Test (\u2191)": 0.979, "Disparagement Sex (\u2191)": 0.325, "Disparagement Race (\u2191)": 0.749, "Prefereence RtA (\u2191)": 0.442}, {"Model": "PaLM2", "Stereotype Recognitio(\u2191)": 0.634, "Overall Agreement Rat(\u2193)": 0.075, "Stereotype Query Test (\u2191)": 0.947, "Disparagement Sex (\u2191)": 0.33, "Disparagement Race (\u2191)": 0.0, "Prefereence RtA (\u2191)": 0.365}]
truthfulness_V2=[{"Model": "Table 7. Internal (\u2191) ", "ChatGPT": 0.288, "GPT-4": 0.417, "ChatGLM2": 0.127, "Vicuna-33b": 0.261, "Vicuna-13b": 0.18, "Vicuna-7b": 0.132, "Llama2-70b": 0.313, "Llama2-13b": 0.235, "Llama2-7b": 0.203, "Wizardlm-13b": 0.19, "Koala-13b": 0.145, "Baichuan-13b": 0.17, "Oasst-12b": 0.101, "ERNIE": 0.255, "Mistral-7b": 0.341, "PaLM2": 0.284}, {"Model": "Table 7. Internal (\u2191) Rank", "ChatGPT": 4.0, "GPT-4": 1.0, "ChatGLM2": 15.0, "Vicuna-33b": 6.0, "Vicuna-13b": 11.0, "Vicuna-7b": 14.0, "Llama2-70b": 3.0, "Llama2-13b": 8.0, "Llama2-7b": 9.0, "Wizardlm-13b": 10.0, "Koala-13b": 13.0, "Baichuan-13b": 12.0, "Oasst-12b": 16.0, "ERNIE": 7.0, "Mistral-7b": 2.0, "PaLM2": 5.0}, {"Model": "Table 9. Externa(\u2191)", "ChatGPT": 0.726, "GPT-4": 0.793, "ChatGLM2": 0.542, "Vicuna-33b": 0.726, "Vicuna-13b": 0.623, "Vicuna-7b": 0.581, "Llama2-70b": 0.721, "Llama2-13b": 0.722, "Llama2-7b": 0.638, "Wizardlm-13b": 0.574, "Koala-13b": 0.553, "Baichuan-13b": 0.622, "Oasst-12b": 0.534, "ERNIE": 0.689, "Mistral-7b": 0.687, "PaLM2": 0.532}, {"Model": "Table 9. Externa(\u2191) Rank", "ChatGPT": 2.0, "GPT-4": 1.0, "ChatGLM2": 14.0, "Vicuna-33b": 2.0, "Vicuna-13b": 9.0, "Vicuna-7b": 11.0, "Llama2-70b": 5.0, "Llama2-13b": 4.0, "Llama2-7b": 8.0, "Wizardlm-13b": 12.0, "Koala-13b": 13.0, "Baichuan-13b": 10.0, "Oasst-12b": 15.0, "ERNIE": 6.0, "Mistral-7b": 7.0, "PaLM2": 16.0}, {"Model": "Table 10. Hallucinatio(\u2191)", "ChatGPT": 0.529, "GPT-4": 0.516, "ChatGLM2": 0.542, "Vicuna-33b": 0.423, "Vicuna-13b": 0.403, "Vicuna-7b": 0.347, "Llama2-70b": 0.402, "Llama2-13b": 0.404, "Llama2-7b": 0.396, "Wizardlm-13b": 0.356, "Koala-13b": 0.451, "Baichuan-13b": 0.306, "Oasst-12b": 0.418, "ERNIE": 0.515, "Mistral-7b": 0.458, "PaLM2": 0.379}, {"Model": "Table 10. Hallucinatio(\u2191) Rank", "ChatGPT": 2.0, "GPT-4": 3.0, "ChatGLM2": 1.0, "Vicuna-33b": 7.0, "Vicuna-13b": 9.0, "Vicuna-7b": 15.0, "Llama2-70b": 10.0, "Llama2-13b": 8.0, "Llama2-7b": 12.0, "Wizardlm-13b": 14.0, "Koala-13b": 6.0, "Baichuan-13b": 16.0, "Oasst-12b": 7.0, "ERNIE": 4.0, "Mistral-7b": 5.0, "PaLM2": 13.0}, {"Model": "Table 13. Persona Sycophanc(\u2193)", "ChatGPT": 0.039, "GPT-4": 0.029, "ChatGLM2": 0.036, "Vicuna-33b": 0.038, "Vicuna-13b": 0.036, "Vicuna-7b": 0.03, "Llama2-70b": 0.043, "Llama2-13b": 0.032, "Llama2-7b": 0.035, "Wizardlm-13b": 0.025, "Koala-13b": 0.04, "Baichuan-13b": 0.032, "Oasst-12b": 0.031, "ERNIE": 0.019, "Mistral-7b": 0.035, "PaLM2": 0.028}, {"Model": "Table 13. Persona Rank", "ChatGPT": 3.0, "GPT-4": 13.0, "ChatGLM2": 5.0, "Vicuna-33b": 4.0, "Vicuna-13b": 5.0, "Vicuna-7b": 12.0, "Llama2-70b": 1.0, "Llama2-13b": 9.0, "Llama2-7b": 7.0, "Wizardlm-13b": 15.0, "Koala-13b": 2.0, "Baichuan-13b": 9.0, "Oasst-12b": 11.0, "ERNIE": 16.0, "Mistral-7b": 7.0, "PaLM2": 4.0}, {"Model": "Table 13. Preference Sycophanc(\u2193)", "ChatGPT": 0.257, "GPT-4": 0.296, "ChatGLM2": 0.432, "Vicuna-33b": 0.458, "Vicuna-13b": 0.375, "Vicuna-7b": 0.395, "Llama2-70b": 0.468, "Llama2-13b": 0.571, "Llama2-7b": 0.587, "Wizardlm-13b": 0.385, "Koala-13b": 0.5, "Baichuan-13b": 0.286, "Oasst-12b": 0.436, "ERNIE": 0.312, "Mistral-7b": 0.293, "PaLM2": 0.581}, {"Model": "Table 13. Preference Rank", "ChatGPT": 1.0, "GPT-4": 4.0, "ChatGLM2": 9.0, "Vicuna-33b": 11.0, "Vicuna-13b": 6.0, "Vicuna-7b": 8.0, "Llama2-70b": 12.0, "Llama2-13b": 14.0, "Llama2-7b": 16.0, "Wizardlm-13b": 7.0, "Koala-13b": 13.0, "Baichuan-13b": 2.0, "Oasst-12b": 10.0, "ERNIE": 5.0, "Mistral-7b": 3.0, "PaLM2": 15.0}, {"Model": "Table 15. Adv Factualit(\u2191)", "ChatGPT": 0.708, "GPT-4": 0.813, "ChatGLM2": 0.349, "Vicuna-33b": 0.699, "Vicuna-13b": 0.665, "Vicuna-7b": 0.469, "Llama2-70b": 0.794, "Llama2-13b": 0.78, "Llama2-7b": 0.718, "Wizardlm-13b": 0.794, "Koala-13b": 0.435, "Baichuan-13b": 0.44, "Oasst-12b": 0.221, "ERNIE": 0.407, "Mistral-7b": 0.426, "PaLM2": 0.273}, {"Model": "Table 15. Adv Factualit(\u2191) Rank", "ChatGPT": 6.0, "GPT-4": 1.0, "ChatGLM2": 14.0, "Vicuna-33b": 7.0, "Vicuna-13b": 8.0, "Vicuna-7b": 9.0, "Llama2-70b": 2.0, "Llama2-13b": 4.0, "Llama2-7b": 5.0, "Wizardlm-13b": 2.0, "Koala-13b": 11.0, "Baichuan-13b": 10.0, "Oasst-12b": 16.0, "ERNIE": 13.0, "Mistral-7b": 12.0, "PaLM2": 15.0}]
safety=[{"Model": "Llama2-13b", "Jailbreak (\u2191)": 0.959, "Toxicity (\u2193)": 0.205, "Misuse (\u2191)": 0.963, "Exaggerated Safety (\u2193)": 0.55}, {"Model": "Llama2-70b", "Jailbreak (\u2191)": 0.974, "Toxicity (\u2193)": 0.248, "Misuse (\u2191)": 0.956, "Exaggerated Safety (\u2193)": 0.315}, {"Model": "Llama2-7b", "Jailbreak (\u2191)": 0.945, "Toxicity (\u2193)": 0.191, "Misuse (\u2191)": 0.943, "Exaggerated Safety (\u2193)": 0.49}, {"Model": "GPT-4", "Jailbreak (\u2191)": 0.914, "Toxicity (\u2193)": 0.386, "Misuse (\u2191)": 0.924, "Exaggerated Safety (\u2193)": 0.085}, {"Model": "ChatGPT", "Jailbreak (\u2191)": 0.898, "Toxicity (\u2193)": 0.352, "Misuse (\u2191)": 0.91, "Exaggerated Safety (\u2193)": 0.15}, {"Model": "ERNIE", "Jailbreak (\u2191)": 0.949, "Toxicity (\u2193)": 0.072, "Misuse (\u2191)": 0.899, "Exaggerated Safety (\u2193)": 0.385}, {"Model": "Wizardlm-13b", "Jailbreak (\u2191)": 0.865, "Toxicity (\u2193)": 0.183, "Misuse (\u2191)": 0.883, "Exaggerated Safety (\u2193)": 0.06}, {"Model": "Vicuna-13b", "Jailbreak (\u2191)": 0.781, "Toxicity (\u2193)": 0.374, "Misuse (\u2191)": 0.848, "Exaggerated Safety (\u2193)": 0.095}, {"Model": "ChatGLM2", "Jailbreak (\u2191)": 0.845, "Toxicity (\u2193)": 0.141, "Misuse (\u2191)": 0.819, "Exaggerated Safety (\u2193)": 0.15}, {"Model": "Koala-13b", "Jailbreak (\u2191)": 0.691, "Toxicity (\u2193)": 0.237, "Misuse (\u2191)": 0.738, "Exaggerated Safety (\u2193)": 0.045}, {"Model": "Vicuna-33b", "Jailbreak (\u2191)": 0.585, "Toxicity (\u2193)": 0.294, "Misuse (\u2191)": 0.735, "Exaggerated Safety (\u2193)": 0.035}, {"Model": "Mistral-7b", "Jailbreak (\u2191)": 0.59, "Toxicity (\u2193)": 0.262, "Misuse (\u2191)": 0.709, "Exaggerated Safety (\u2193)": 0.46}, {"Model": "Oasst-12b", "Jailbreak (\u2191)": 0.69, "Toxicity (\u2193)": 0.154, "Misuse (\u2191)": 0.583, "Exaggerated Safety (\u2193)": 0.05}, {"Model": "Vicuna-7b", "Jailbreak (\u2191)": 0.596, "Toxicity (\u2193)": 0.213, "Misuse (\u2191)": 0.565, "Exaggerated Safety (\u2193)": 0.09}, {"Model": "PaLM 2", "Jailbreak (\u2191)": 0.486, "Toxicity (\u2193)": 0.317, "Misuse (\u2191)": 0.473, "Exaggerated Safety (\u2193)": 0.377}, {"Model": "Baichuan-13b", "Jailbreak (\u2191)": 0.25, "Toxicity (\u2193)": 0.112, "Misuse (\u2191)": 0.114, "Exaggerated Safety (\u2193)": 0.19}]
