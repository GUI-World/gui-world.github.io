<!doctype html>
<html>

<head>
  <title>GUI-World: A Dataset for GUI-Orientated Multimodal Large Language Models</title>
  <link rel="icon" href="img/GUI-world-logo.jpg" type="image/icon type">
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="/dist/output.css" rel="stylesheet">
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <script src="js/footer.js"></script>
  <script src="js/index.js"></script>

  <style>

  </style>
</head>





<body>

  <div class="menu-container"></div>
<div class="fullscreen-background">
  <img src="img/website-background_00.png"> <!-- 更改为你的图片路径 -->
  <div class="gradient-text"> GUI-World: A Dataset for GUI-Orientated Multimodal Large Language Models</div>
</div>
  <div id="body">
  <div class="content-container">
    <div class="content">
      <div class="content-table flex-column">

        <h1 class="supportTitle">Introduction</h1>
        <br>
        <hr>
        <div class="arch">
          <img src="img/GUI_overview.png" class="title-icon2" alt="Rank Card">
        </div>
        <div class="flex-row">
          <div class="flex-item flex-item-stretch-4 flex-column">
            <p class="text">
              In this work, we introduce a comprehensive GUI-orientated dataset, GUI-World, to benchmark and enhance GUI understanding capabilities. Specifically, there are three-fold major contributions:
              <ol>
                <li><b>A Dataset.</b> We propose GUI-WORLD, a comprehensive GUI dataset comprising over 12,000 videos specifically designed to assess and improve the GUI understanding capabilities of MLLMs, spanning a range of categories and scenarios, including desktop, mobile, and extended reality (XR), and representing the first GUI-oriented instruction-tuning dataset in the video domain. </li>
                <li><b>A Novel Model.</b> Based on <b>GUI-WORLD</b>, we propose <b>GUI-Vid</b>, a GUI-orientated VideoLLM with enhanced capabilities to handle various and complex GUI tasks. <b>GUI-Vid</b> shows a significant improvement on the benchmark and achieves results comparable to the top-performing models </li>
                <li><b>Comprehensive Experiments and Valuable Insights.</b> Our experiments indicate that most existing MLLMs continue to face challenges with GUI-oriented tasks, particularly in sequential and dynamic GUI content. Empirical findings suggest that improvements in vision perception, along with an increase in the number of keyframes and higher resolution, can boost performance in GUI-oriented tasks, thereby paving the way for the future of GUI agents.</li>
              </ol>
            </p>
          </div>
        </div>

        <h1 class="supportTitle">Empirical Findings</h1>
        <br>

        <h2 class="supportTitle2">Overall Assessments</h2>

        <div class="featurecard-container">
          <!-- Trustworthiness and Utility -->
          <div class="feature_1x1">
            <div class="featurecard">
              <h2>Commercial ImageLLMs outperform Open-source VideoLLMs in Zero-shot Settings</h2>
            </div>
            <div class="description">
              <p><ol>
                  <li><b>Commercial ImageLLMs, notably GPT-4V and GPT-4o, consistently outperform open-source VideoLLMs in zero-shot settings. 
                    GPT-4o exhibits superior performance across all GUI scenarios in complex tasks, reflected in its high scores in both multiple-choice and free-form queries, with an average of 84.8% and 3.573. 
                    Similarly, Gemini demonstrates strong capabilities in captioning and descriptive tasks within software and iOS environments, scoring 2.836 and 2.936, respectively. 
                    Further analysis reveals that GPT-4V excels in applications with minimal textual content and simple layouts, such as TikTok, health apps, and GitHub. 
                    In contrast, its performance drops in more intricate applications like Microsoft ToDo and XR software. 
                    As for VideoLLMs, their significantly poorer performance is attributed to two main factors: their inability to accurately interpret GUI content from user inputs and a lack of sufficient GUI-oriented pretraining, which is evident from their inadequate performance in basic captioning and description tasks.</li>
              </ol></p>
            </div>
          </div>

          <!-- Alignment of LLMs -->
          <div class="feature_1x1">
            <div class="featurecard">
              <h2>Performance Variate in Different GUI Scenarios</h2>
            </div>
            <div class="description">
              <p>
                GPT-4V and Gemini excel in common scenarios such as mobile and website interfaces but show marked deficiencies in more complex GUI environments like XR and multi-window interactions, across both captioning and intricate tasks. 
                This performance gap highlights a significant shortfall in understanding environments where GUI elements are scattered and demand sophisticated interpretation. 
                It emphasizes the critical need for specialized benchmarks and datasets tailored to these complex GUI scenarios, which is essential for enhancing the GUI-oriented capabilities of MLLMs, paving the way for them to become truly reliable and high-performing general control agents.
              </p>
            </div>
          </div>

          <!-- Performance Gap in Trustworthiness -->
          <div class="feature_1x1">
            <div class="featurecard">
              <h2>Keyframe Selection is Important for GUI-orientated Tasks</h2>
            </div>
            <div class="description">
              <p>
                Across both basic tasks such as captioning and more complex tasks like prediction and reasoning, significant variations are evident among keyframe selection methods. 
                GPT-4V and Gemini significantly benefit from using random-selected and human-selected keyframes, scoring approximately 0.2-0.3 points higher in both captioning and free-form tasks than those using programmatic extraction. 
                This suggests that traditional keyframe technologies, designed for natural videos, are less effective for detecting essential GUI operations, particularly when subtle movements like mouse clicks and dynamic changes are involved. 
                Conversely, the difference in performance is relatively smaller in Qwen-VL-Max, indicating that while keyframe selection methods are crucial for models proficient in GUI content, they exert less influence on less capable models.
              </p>
            </div>
          </div>


          <!-- Transparency in Trustworthiness -->
          <div class="feature_1x1">

            <div class="featurecard">
              <h2>Dynamic GUI Tasks Continue to Challenge MLLMs</h2>
            </div>
            <div class="description">
              <p>
                In the fine-grained tasks, GPT-4V and GPT-4o excel with static GUI content and prediction tasks over image sequences but struggle with providing detailed descriptions for entire videos and dynamic GUI content. 
                This discrepancy is attributed to minor variations in GUI that significantly impact descriptions. Enhancing the number of keyframes and the granularity of perception might mitigate these issues. 
                Among VideoLLMs, ChatUnivi excels in conversational tasks by effectively leveraging contextual nuances, particularly in subsequent rounds, yet it underperforms in GUI-oriented captioning tasks. 
                In contrast, GUI-Vid demonstrates proficiency in sequential tasks but falls short in both captioning and static content handling. 
                This gap is linked to deficiencies in GUI-Vid’s pretraining, which lacked comprehensive GUI content crucial for effective vision-text alignment, as evidenced by its poor performance and an instruction tuning process also failed to fully address these shortcomings.</p>

            </div>
          </div>
          <div class="feature_1x1">

            <div class="featurecard">
              <h2>Vision Perception is Important for Sequential GUI Tasks</h2>
            </div>
            <div class="description">
              <p>
                Integrating detailed textual information slightly outperforms purely vision-based inputs or detailed captions, akin to a Chain of Thought (CoT) setting. 
                Surprisingly, GPT-4V excels in caption and prediction tasks with just detailed captions, providing insights on enhancing specific GUI-oriented tasks through additional textual information. 
                However, it still falls short in more challenging tasks, such as retrieving static or dynamic content. This underscores the critical role of visual perception in GUI environments, where even minor changes can significantly impact outcomes.</p>

            </div>
          </div>
          <div class="feature_1x1">

            <div class="featurecard">
              <h2>Supreme Enhancement of GUI-Vid on Graphic-based Interface After Finetuned on GUI-World</h2>
            </div>
            <div class="description">
              <p>
                As a pioneering study in training VideoLLMs as screen agents, GUI-Vid significantly outperforms the baseline model, showing an average improvement of 30% across various tasks and GUI scenarios, even surpassing the commercial ImageLLM, Qwen-VL-Max. 
                This enhancement is particularly notable in captioning and prediction over image sequences, where GUI-Vid matches the performance of GPT-4V and Gemini-Pro. Our two-stage progressive fintuning significantly enhances the performance in all GUI scenarios. 
                Remarkably, GUI-Vid scored 3.747 in caption tasks within the XR scenario, highlighting its potential in XR applications and the high-quality annotations provided by our dataset. 
                However, in Multiple-Choice QA and Chatbot tasks, GUI-Vid still lags behind industry leaders like GPT-4V and Gemini-Pro, a discrepancy likely due to the baseline LLM’s weaker performance and the challenges of instruction-based fine-tuning.</p>

            </div>
          </div>
          <div class="feature_1x1">

            <div class="featurecard">
              <h2>Upper Bound of GUI-orientated Capability with More Keyframes and High Resolution</h2>
            </div>
            <div class="description">
              <p>
                Our two ablation studies during the fine-tuning phase demonstrate that utilizing GUI image-text captioning data significantly enhances the model's preliminary understanding of GUI elements, outperforming training that relies solely on videos. 
                Additionally, an increased number of keyframes correlates with improved performance across various scenarios, notably in environments featuring multiple windows and software applications. 
                Further evidence reveals that higher image resolutions substantially boost task performance, both basic and complex, for GPT-4o. 
                These findings underscore the potential for further developing a more robust GUI Agent.</p>

            </div>
          </div>
        </div>




        <br>
        <!--          <div class="feature">-->
        <!--            <h2>Taxonomy for TrustLLM</h2>-->
        <!--            <div class="flourish-embed flourish-hierarchy" data-src="visualisation/15357314">-->
        <!--              <script src="https://public.flourish.studio/resources/embed.js"></script>-->
        <!--            </div>-->
        <!--          </div>-->


      </div>

      <h1 class="supportTitle">Models</h1>

    </div>
      <div class="content">
        <div class="content-table flex-column">

          <br>
       

          <div class="flex-row">
            <div class="custom-table-container center add-top-margin-small">
              <table class="custom-table">
                <thead>
                  <tr>
                    <th>Model</th>
                    <th>Model Size</th>
                    <th>Open-Weight</th>
                    <th>Version</th>
                    <th>Creator</th>
                    <th>Source</th>
                    <th>Link</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td data-title="Model">GPT-4o</td>
                    <td data-title="Model Size">unknown</td>
                    <td data-title="Open-Weight"><span class="bg-yellow-100 text-yellow-700 font-bold py-1 px-3 rounded-full">
                      No
                    </span>
                    <td data-title="Version">-</td>
                    <td data-title="Creator">OpenAI</td>
                    <td data-title="Source">OpenAI API</td>
                    <td data-title="Link">
                                  <a href="https://openai.com/index/hello-gpt-4o/" target="_blank">
                      <button class="custom-button-flat"><img src="img/openai.png"></button>
                      </a>
                    </td>
                  </tr>
                  <tr>
                  <tr>
                    <td data-title="Model">GPT-4V(ision)</td>
                    <td data-title="Model Size">unknown</td>
                    <td data-title="Open-Weight"><span class="bg-yellow-100 text-yellow-700 font-bold py-1 px-3 rounded-full">
                      No
                    </span>
                    <td data-title="Version">-</td>
                    <td data-title="Creator">OpenAI</td>
                    <td data-title="Source">OpenAI API</td>
                    <td data-title="Link">
                                  <a href="https://openai.com/research/gpt-4v-system-card" target="_blank">
                      <button class="custom-button-flat"><img src="img/openai.png"></button>
                      </a>
                    </td>
                  </tr>
                  <tr>
                    <td data-title="Model">Gemini-Pro-1.5</td>
                    <td data-title="Model Size">unknown</td>
                    <td data-title="Open-Weight"><span class="bg-yellow-100 text-yellow-700 font-bold py-1 px-3 rounded-full">
                      No

                    </span>

                    <td data-title="Version">v1.5</td>
                    <td data-title="Creator">Google</td>
                    <td data-title="Source">Google API</td>
                    <td data-title="Link">
                                  <a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-pro-vision" target="_blank">
                      <button class="custom-button-flat"><img src="img/palm2.png"></button>
                      </a>
                    </td>
                  </tr>
                  <tr>
                    <td data-title="Model">Qwen-VL-Max</td>
                    <td data-title="Model Size">unknown</td>
                    <td data-title="Open-Weight"><span class="bg-yellow-100 text-yellow-700 font-bold py-1 px-3 rounded-full">
                      No
                    </span>
                    <td data-title="Version">-</td>
                    <td data-title="Creator">Ali</td>
                    <td data-title="Source">Ali API</td>
                    <td data-title="Link">
                      <a href="https://github.com/QwenLM/Qwen-VL" target="_blank">
                      <button class="custom-button-flat"><img src="img/ali.png"></button>
                      </a>
                      </td>
                  </tr>

              

                  <tr>
                    <td data-title="Model">Videochat2</td>
                    <td data-title="Model Size">7b</td>
                                        <td data-title="Open-Weight"><span class="bg-green-100 text-green-700 font-bold py-1 px-3 rounded-full">
                      Yes
                    </span>
                    <td data-title="Version">v2</td>
                    <td data-title="Creator">OpenGVLab</td>
                    <td data-title="Source">HuggingFace</td>
                    <td data-title="Link">
                    <a href="https://github.com/OpenGVLab/Ask-Anything" target="_blank">
                      <button class="custom-button-flat"><img src="img/videochat2-logo.png"></button>
                      </a>
                      </td>
                  </tr>
                  <tr>
                    <td data-title="Model">ChatUnivi</td>
                    <td data-title="Model Size">7b</td>
                                        <td data-title="Open-Weight"><span class="bg-green-100 text-green-700 font-bold py-1 px-3 rounded-full">
                      Yes
                    </span>
                    <td data-title="Version">v1.0</td>
                    <td data-title="Creator">PKU-YuanGroup</td>
                    <td data-title="Source">HuggingFace</td>
                    <td data-title="Link">
                    <a href="https://github.com/PKU-YuanGroup/Chat-UniVi" target="_blank">
                      <button class="custom-button-flat"><img src="img/chatunivi-logo.png"></button>
                      </a>
                      </td>
                  </tr>
                  <tr>
                    <td data-title="Model">MiniGPT4Video</td>
                    <td data-title="Model Size">7b</td>
                                        <td data-title="Open-Weight"><span class="bg-green-100 text-green-700 font-bold py-1 px-3 rounded-full">
                      Yes
                    </span>
                    <td data-title="Version">-</td>
                    <td data-title="Creator">Vision-CAIR</td>
                    <td data-title="Source">HuggingFace</td>
                    <td data-title="Link">
                    <a href="https://github.com/THUDM/CogVLM" target="_blank">
                      <button class="custom-button-flat"><img src="img/minigpt4video-logo.png"></button>
                      </a>
                      </td>
                  </tr>
                  
                  <!-- ...continue for other rows... -->
                  <!-- Repeat for other rows, alternating color for LightCyan rows -->
                </tbody>
              </table>
            </div>
          </div>
        </div>
      </div>


    <h1 class="supportTitle">GUI-World Dataset Details</h1>

    <div class="content">
      <div class="content-table flex-column">
        <br>
        <div class="flex-row">
          <div class="custom-table-container center add-top-margin-small">
            <table class="custom-table">
              <caption></caption>
              <thead>
                <tr>
                    <th>Category</th>
                    <th>Total Videos</th>
                    <th>Free-form</th>
                    <th>MCQA</th>
                    <th>Conversation</th>
                    <th>Total Frame. (Avg.)</th>
                    <th>Avg. Anno.</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Software</td>
                    <td>4,720</td>
                    <td>27,840</td>
                    <td>9,440</td>
                    <td>9,440</td>
                    <td>23,520 (4.983)</td>
                    <td>7.558</td>
                </tr>
                <tr>
                    <td>Website</td>
                    <td>2,499</td>
                    <td>14,994</td>
                    <td>4,998</td>
                    <td>4,998</td>
                    <td>15,371 (6.151)</td>
                    <td>6.862</td>
                </tr>
                <tr>
                    <td>IOS</td>
                    <td>492</td>
                    <td>2,952</td>
                    <td>984</td>
                    <td>984</td>
                    <td>2,194 (4.459)</td>
                    <td>7.067</td>
                </tr>
                <tr>
                    <td>Multi</td>
                    <td>475</td>
                    <td>2,850</td>
                    <td>950</td>
                    <td>950</td>
                    <td>2,507 (5.277)</td>
                    <td>7.197</td>
                </tr>
                <tr>
                    <td>XR</td>
                    <td>393</td>
                    <td>2,358</td>
                    <td>786</td>
                    <td>786</td>
                    <td>1,584 (4.030)</td>
                    <td>10.970</td>
                </tr>
                <tr>
                    <td>Android</td>
                    <td>3,800</td>
                    <td>15,199</td>
                    <td>7,600</td>
                    <td>7,600</td>
                    <td>38,000 (10.000)</td>
                    <td>-</td>
                </tr>
                <tr>
                    <td>Summary</td>
                    <td>12,379</td>
                    <td>76,673</td>
                    <td>24,758</td>
                    <td>24,758</td>
                    <td>83,176 (6.719)</td>
                    <td>7.463</td>
                </tr>
            </tbody>
            </table>
          </div>
        </div>
        <br><br>


  
  <h1 class="supportTitle">Citation</h1>
  <br>
  <div class="content">
  <div class="flex-row">
    <div class="flex-item flex-item-stretch-4 flex-column">

  <pre class="bibtax">
    @article{chen2024gui,
      title={GUI-WORLD: A Dataset for GUI-Orientated Multimodal Large Language Models},
      author={GUI-World Team},
      year={2024}
    }
    </pre>
  <br>
  </div>
</div>
  </div>

  <div class="content">
  <div id="supportContainer">
    <h1 class="supportTitle">GUI-World Team</h1>
    <br>

    <div id="logoContainer">
      <img src="img/logos/HUST.png" alt="School 1" class="schoolLogo">
      <img src="img/logos/Lehigh-University-logo.png" alt="School 2" class="schoolLogo">
      <img src="img/logos/Microsoft.png" alt="School 3" class="schoolLogo">
      
    </div>


  </div>
</div>
  <div class="footer-container"></div>
  </div>
  </div>
</body>

</html>